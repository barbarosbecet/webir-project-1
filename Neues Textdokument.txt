def crawl(urls, level=1):
    """
    Crawling for links and BibTex files on the specified URLs to the deepness of level
    """
    links = []
    all_links = [[], []]
    for url in range(len(urls)):
        with urlopen(urls[url]) as response:
            bs = BeautifulSoup(response, "html.parser")
            for lnk in bs.findAll("a", href=re.compile(".*bib$")):
                if "href" in lnk.attrs:
                    links.append(lnk.attrs["href"])
        all_links[0] = get_links(urls[url])

        # belongs to the hop
        for lvl in range(1, level):
            for ins in range(len(all_links[lvl-1])):
                all_links_tmp = []
                all_links_tmp = get_links(all_links[lvl-1][ins])
            all_links[lvl-1] = all_links_tmp

    for lvl in range(1, level):
        for ln in range(1, len(all_links[lvl])):
            with urlopen(all_links[lvl][ln]) as response:
                bs = BeautifulSoup(response, "html.parser")
                for lnk in bs.findAll("a", href=re.compile(".*bib$")):
                    if "href" in lnk.attrs:
                        links.append(lnk.attrs["href"])

    return links